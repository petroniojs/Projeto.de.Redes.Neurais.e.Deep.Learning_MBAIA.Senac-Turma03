# -*- coding: utf-8 -*-
"""Trabalho Final_Deep Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EpCCHL_WMa6eBTkLrPLIeaQdvcOF0vsx

# **Instalando as biblotecas necessárias e entendendo a base de dados**
"""

# Instalar pacotes necessários (execute uma vez)
!pip install ucirepo scikit-learn tensorflow matplotlib seaborn --quiet

# 1. Carregamento e exploração inicial
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from ucimlrepo import fetch_ucirepo

# Carregar o dataset
cov = fetch_ucirepo(id=31)
df  = cov.data.features.copy()
df['Cover_Type'] = cov.data.targets.values  # adiciona a coluna-alvo ao DataFrame

# Dicionário de tradução para nomes de colunas (variáveis contínuas)
colunas_traducao = {
    'Elevation': 'Elevacao',
    'Aspect': 'Orientacao',
    'Slope': 'Declinacao',
    'Horizontal_Distance_To_Hydrology': 'Dist_Horiz_Hidrologia',
    'Vertical_Distance_To_Hydrology': 'Dist_Vert_Hidrologia',
    'Horizontal_Distance_To_Roadways': 'Dist_Horiz_Estradas',
    'Hillshade_9am': 'Sombreamento_9h',
    'Hillshade_Noon': 'Sombreamento_12h',
    'Hillshade_3pm': 'Sombreamento_15h',
    'Horizontal_Distance_To_Fire_Points': 'Dist_Horiz_Pontos_Fogo'
}

# Se desejar, renomear também Wilderness_Area_* e Soil_Type_* individualmente:
# Wilderness_Area_1..4 -> Area_Selvagem_1..4
colunas_traducao.update({f'Wilderness_Area_{i}': f'Area_Selvagem_{i}' for i in range(1, 5)})
# Soil_Type_1..40 -> Tipo_Solo_1..40
colunas_traducao.update({f'Soil_Type_{i}': f'Tipo_Solo_{i}' for i in range(1, 41)})

# Aplicar a renomeação
df.rename(columns=colunas_traducao, inplace=True)

# Traduzir os rótulos das classes (1–7) para nomes de cobertura florestal em português
mapeamento_cover = {
    1: 'Abeto/Picea (Spruce/Fir)',
    2: 'Pinheiro Lodgepole',
    3: 'Pinheiro Ponderosa',
    4: 'Choupo/Salgueiro',
    5: 'Álamo/Tremendo (Aspen)',
    6: 'Pinheiro-de-Douglas',
    7: 'Krummholz (vegetação rasteira alpina)'
}
df['Cover_Type'] = df['Cover_Type'].map(mapeamento_cover)

# Verificar o resultado
display(df.head())

# Contagem por tipo de cobertura
display(df['Cover_Type'].value_counts())

# Obter informações sobre tipos de dados e ausências
df.info()

# Exibir estatísticas descritivas das variáveis contínuas (agora renomeadas)
cont_cols = ['Elevacao','Orientacao','Declinacao','Dist_Horiz_Hidrologia',
             'Dist_Vert_Hidrologia','Dist_Horiz_Estradas',
             'Sombreamento_9h','Sombreamento_12h','Sombreamento_15h',
             'Dist_Horiz_Pontos_Fogo']
display(df[cont_cols].describe())

"""## **Pré-processamento**"""

# Separar X (features) e y (rótulo)
X = df.drop('Cover_Type', axis=1)
y = df['Cover_Type']

# Identificar colunas contínuas (primeiras 10 de acordo com a documentação do UCI)
cont_cols = ['Elevacao','Orientacao','Declinacao','Dist_Horiz_Hidrologia',
             'Dist_Vert_Hidrologia','Dist_Horiz_Estradas',
             'Sombreamento_9h','Sombreamento_12h','Sombreamento_15h',
             'Dist_Horiz_Pontos_Fogo']

# Dividir dados em treino e teste inicial (70/30)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)
# Dividir o conjunto temporário em validação e teste (15/15)
X_val, X_test, y_val, y_test   = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42)

# Escalar variáveis contínuas usando StandardScaler
scaler = StandardScaler()
X_train[cont_cols] = scaler.fit_transform(X_train[cont_cols])
X_val[cont_cols]   = scaler.transform(X_val[cont_cols])
X_test[cont_cols]  = scaler.transform(X_test[cont_cols])

"""# **Modelando a construção de uma MLP**"""

from sklearn.preprocessing import LabelEncoder

def build_model(units_layer1=64, units_layer2=32, activation='relu', learning_rate=0.001, l2_reg=0.0, dropout_rate=0.0):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(units_layer1, activation=activation,
                                    kernel_regularizer=tf.keras.regularizers.l2(l2_reg),
                                    input_shape=(X_train.shape[1],)))
    if dropout_rate > 0.0:
        model.add(tf.keras.layers.Dropout(dropout_rate))
    model.add(tf.keras.layers.Dense(units_layer2, activation=activation,
                                    kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))
    if dropout_rate > 0.0:
        model.add(tf.keras.layers.Dropout(dropout_rate))
    model.add(tf.keras.layers.Dense(7, activation='softmax'))  # 7 classes - Já qe são 7 tipos de vegetação

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Encode target variables
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_val_encoded = le.transform(y_val)
y_test_encoded = le.transform(y_test)


# Construir modelo baseline
baseline_model = build_model()

# Treinar com EarlyStopping
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history_baseline = baseline_model.fit(
    X_train, y_train_encoded, # Use encoded y_train
    epochs=20,
    batch_size=128,
    validation_data=(X_val, y_val_encoded), # Use encoded y_val
    callbacks=[early_stop],
    verbose=2
)

"""A acurácia de validação começou em torno de 77,99% e subiu gradualmente até cerca de 86,48% na última época. Isso indica que o modelo está aprendendo a classificar os tipos de cobertura florestal e melhorando a performance a cada época.

Testamos algmas quantdades de épocas e escolhemos 20 como ponto de partida razoável:

Com uma arquitetura simples e funções de ativação como ReLU, observou-se que a
acurácia de validação estabiliza em torno da décima quarta época. Continuar treinando indefinidamente pode levar ao overfitting, em que o modelo memoriza o conjunto de treino e perde desempenho no conjunto de teste.

Além disso, samos EarlyStopping para monitorar a perda de validação; se ela não melhora por algumas épocas, o callback interrompe o treinamento antes de atingir o número máximo. Assim, definir 20 ou 30 épocas dá margem para que o treinamento pare mais cedo quando for o caso, sem desperdiçar tempo.

# **Avaliando o modelo e visualizando as curvas de aprendizagem**
"""

# Função de plot das curvas
def plot_learning_curves(history, title='Curvas de Aprendizagem'):
    epochs = range(1, len(history.history['loss']) + 1)
    plt.figure(figsize=(12,4))
    plt.subplot(1,2,1)
    plt.plot(epochs, history.history['loss'], label='Treino')
    plt.plot(epochs, history.history['val_loss'], label='Validação')
    plt.title('Loss por Época')
    plt.xlabel('Épocas'); plt.ylabel('Loss'); plt.legend()

    plt.subplot(1,2,2)
    plt.plot(epochs, history.history['accuracy'], label='Treino')
    plt.plot(epochs, history.history['val_accuracy'], label='Validação')
    plt.title('Acurácia por Época')
    plt.xlabel('Épocas'); plt.ylabel('Acurácia'); plt.legend()
    plt.suptitle(title)
    plt.show()

# Plotar curvas do baseline
plot_learning_curves(history_baseline, 'Baseline')

# Avaliação no conjunto de teste
test_preds = np.argmax(baseline_model.predict(X_test), axis=1)
print(classification_report(y_test_encoded, test_preds, digits=3))
print(confusion_matrix(y_test_encoded, test_preds))

"""No geral, a análse confirma que a MLP básica aprende bem com os dados disponíveis e generaliza de forma estável.

Loss por época: as curvas de perda de treino e de validação começam em valores relativamente altos (~0,60 e ~0,52) e caem de forma suave e consistente. A partir da 7ª/8ª época, elas praticamente se sobrepõem e continuam caindo até cerca de 0,35 na 20ª época. O fato de a perda de validação acompanhar de perto a de treino, sem divergência significativa, indica que o modelo está generalizando bem; não há sinais de que esteja memorizando o conjunto de treino.

Acurácia por época: ambas as curvas sobem rapidamente nas primeiras épocas, saindo de ~0,77 para 0,86. Ao longo do treinamento as linhas permanecem próximas uma da outra, com pequenas oscilações normais. A acurácia de validação chega a ficar um pouco acima da acurácia de treino em alguns pontos (por exemplo, por volta da época 14), mas a diferença é mínima e as curvas convergem para valores semelhantes no final. Isso reforça que não há overfitting: o modelo consegue manter desempenho semelhante nos dados não vistos.

Estabilidade: não há grandes saltos ou oscilações abruptas, o que sugere que a taxa de aprendizado e o tamanho do lote estão adequados. As pequenas variações na val_loss e val_accuracy nas épocas finais são normais e não indicam overfitting.

Confirmação do comportamento: o relatório não mostra nenhum sinal de overfitting (como precision 100% e recall muito baixo ou vice-versa). As métricas são consistentes com as curvas de perda/ acurácia: há melhorias contínuas, mas não há grandes discrepâncias entre treino e teste.

# **Experimentos com hiperparâmetros**

**1. Rede mas profunda (128→64→32 neurônios, ReLU)**
"""

def build_deep_relu_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(X_train.shape[1],)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(7, activation='softmax')
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

deep_relu_model = build_deep_relu_model()
history_deep_relu = deep_relu_model.fit(
    X_train, y_train_encoded, # Use encoded y_train
    epochs=30,
    batch_size=128,
    validation_data=(X_val, y_val_encoded), # Use encoded y_val
    callbacks=[early_stop],
    verbose=2
)

# Avaliação
test_preds_deep_relu = np.argmax(deep_relu_model.predict(X_test), axis=1)
print(classification_report(y_test_encoded, test_preds_deep_relu, digits=3))
plot_learning_curves(history_deep_relu, 'Deep ReLU (3 camadas)')

"""**2. Função de ativação diferente (tanh)**"""

def build_tanh_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(X_train.shape[1],)),
        tf.keras.layers.Dense(128, activation='tanh'),
        tf.keras.layers.Dense(64, activation='tanh'),
        tf.keras.layers.Dense(32, activation='tanh'),
        tf.keras.layers.Dense(7, activation='softmax')
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

tanh_model = build_tanh_model()
history_tanh = tanh_model.fit(
    X_train, y_train_encoded, # Use encoded y_train
    epochs=30,
    batch_size=128,
    validation_data=(X_val, y_val_encoded), # Use encoded y_val
    callbacks=[early_stop],
    verbose=2
)

test_preds_tanh = np.argmax(tanh_model.predict(X_test), axis=1)
print(classification_report(y_test_encoded, test_preds_tanh, digits=3))
plot_learning_curves(history_tanh, 'Ativação tanh (3 camadas)')

"""Não exstem grandes separações entre as curvas de treno e de validação. sso sugere que não há overfitting

**4. Taxa de aprendizado diferente (learning_rate = 0,01)**
"""

def build_high_lr_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(X_train.shape[1],)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(7, activation='softmax')
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),  # taxa de aprendizado alta
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

high_lr_model = build_high_lr_model()
history_high_lr = high_lr_model.fit(
    X_train, y_train_encoded,
    epochs=30,
    batch_size=128,
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stop],
    verbose=2
)

test_preds_high_lr = np.argmax(high_lr_model.predict(X_test), axis=1)
print(classification_report(y_test_encoded, test_preds_high_lr, digits=3))
plot_learning_curves(history_high_lr, 'Learning Rate 0.01')

"""**5. Tamanho de batch maior e mais épocas**

o batch de 512 foi testado por ser uma escolha técnica comum e computacionalmente eficiente, mas seus resultados mostraram que ele não melhora o desempenho nesse caso específico
"""

def build_large_batch_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(X_train.shape[1],)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(7, activation='softmax')
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

large_batch_model = build_large_batch_model()
history_large_batch = large_batch_model.fit(
    X_train, y_train_encoded,
    epochs=40,             # número maior de épocas
    batch_size=512,        # batch size maior
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stop],
    verbose=2
)

test_preds_large_batch = np.argmax(large_batch_model.predict(X_test), axis=1)
print(classification_report(y_test_encoded, test_preds_large_batch, digits=3))
plot_learning_curves(history_large_batch, 'Batch 512 + 40 épocas')

"""**Comparando os resultados**"""

import pandas as pd

reports = {
    'baseline': classification_report(y_test_encoded, baseline_model.predict(X_test).argmax(axis=1), output_dict=True),
    'deep_relu': classification_report(y_test_encoded, test_preds_deep_relu, output_dict=True),
    'tanh': classification_report(y_test_encoded, test_preds_tanh, output_dict=True),
    'lr_0.01': classification_report(y_test_encoded, test_preds_high_lr, output_dict=True),
    'batch_512': classification_report(y_test_encoded, test_preds_large_batch, output_dict=True),
}

summary = pd.DataFrame({
    name: {
        'accuracy': rep['accuracy'],
        'macro_f1': sum(rep[str(i)]['f1-score'] for i in range(7))/7 # Note: class labels are 0-6 for encoded data
    }
    for name, rep in reports.items()
}).T

display(summary)

"""Conclusões a partir do s comparativos:

Tanh: foi a configuração que mais se destacou, com acurácia em torno de 92,5% e F1 macro de 0,873. Isso sugere que substituir a ReLU por tanh nas camadas ocultas ajudou a rede a capturar melhor as relações entre as variáveis e a classificar as classes menos frequentes, elevando tanto o desempenho geral quanto a média de F1.

Deep ReLU: o modelo mais profundo (128→64→32 neurônios, todas com ReLU) melhorou consideravelmente em relação ao baseline, alcançando acurácia de ~89,5% e F1 macro de 0,839. A maior profundidade permite representar padrões mais complexos, mas não chegou ao mesmo nível do tanh.

Baseline: mantém acurácia de ~85,6% e F1 macro de 0,779, servindo como referência inicial.

Aprendizado com taxa alta (lr_0.01): com learning rate elevado (0,01), o modelo não melhora, ficando praticamente no nível do baseline (acurácia ~86,1%, F1 macro ~0,771). Isso indica que o passo de atualização grande demais dificultou a convergência.

Batch grande (batch_512): usar batch size de 512 e mais épocas produziu desempenho equivalente ao baseline (acurácia ~85,6% e F1 macro ~0,775), sugerindo que lotes muito grandes podem reduzir a variabilidade do gradiente sem ganhos de generalização.

De acordo com esses insigths,  profundidade e escolha da função de ativação são os fatores que mais impactaram positivamente a performance. As configurações com tanh e com rede mais profunda apresentaram os maiores ganhos, tanto em acurácia quanto em F1 macro, enquanto ajustes isolados na taxa de aprendizado ou tamanho do batch não trouxeram melhorias.
"""